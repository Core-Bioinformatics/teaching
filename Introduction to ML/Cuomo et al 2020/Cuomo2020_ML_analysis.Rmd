---
title: "Single Cell RNA-seq Classification - Cuomo et al 2020"
output: html_document
---

The single-cell RNA-seq data used in this example comes from the Cuomo et al 2020 paper (https://www.nature.com/articles/s41467-020-14457-z). The expression levels have been normalised using SCTransform (https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1). Cells were clustered using Seurat (https://satijalab.org/seurat/). Here, we are only considering cells from day 3, coming from donors hayt, melw and naah. 500 (non-constant) genes have been selected at random to lower the time requirements. We will attempt to classify cells into 3 predefined cell types.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load libraries, message=FALSE,warning=FALSE}
if(!require(caret)){install.packages("caret")}
library(caret)
if(!require(ggplot2)){install.packages("ggplot2")}
library(ggplot2)
if(!require(UpSetR)){install.packages("UpsetR")}
library(UpSetR)
if(!require(GGally)){install.packages("GGally")}
library(GGally)
if(!require(rpart.plot)){install.packages("rpart.plot")}
library(rpart.plot)
if(!require(Rtsne)){install.packages("Rtsne")}
library(Rtsne)
if(!require(dplyr)){install.packages("dplyr")}
library(dplyr)
if(!require(randomForest)){install.packages("randomForest")}
library(randomForest)
```

## Loading data

```{r loading data}
cuomo = read.csv('Cuomo2020_ML.csv',row.names = 1)
cuomoData = cuomo[,colnames(cuomo)[colnames(cuomo)!='classification']]
cuomoClass = cuomo$classification
```

## Initial visualisation

Pick a few genes and plot the distribution of expression for that gene in each cluster.

```{r density plots}
set.seed(42)
some.genes = c("YBX1","BANF1","S100A16","MYO1E","AC105460.2")
for (gene in some.genes){
  plot.df = data.frame('cluster'=as.factor(cuomoClass),'gene'=cuomoData[,gene])
  print(ggplot(plot.df,aes(x=gene,y=..density..,color=cluster))+geom_density()+ggtitle(gene)+theme_bw())
}  

```
```{r ggpairs,message=FALSE}
pairs.df = cuomo[,c(some.genes,'classification')]
pairs.df$classification = as.factor(pairs.df$classification)

#pdf('ggpairs_5genes.pdf', width=10, height=10)
ggpairs(pairs.df,mapping = aes(colour=classification,alpha=0.5))
#dev.off()
```
```{r tsne}
cuomoTsne <- Rtsne(cuomoData, check_duplicates=FALSE, pca=TRUE, perplexity=50, theta=0.5, dims=2)
print(cuomoTsne$Y %>% 
  as.data.frame() %>% 
  rename(tSNE1=V1,  tSNE2=V2) %>% 
  mutate( samples=as.character(cuomoClass) )%>% 
  ggplot() +
  geom_point( mapping = aes(x=tSNE1, y=tSNE2, color=samples), alpha=0.5) +
  theme_bw())
```

## Preprocessing

### Highly correlated variables

Identify highly correlated variables.

```{r correlation}
corMat <- cor(cuomoData)
highCorr <- findCorrelation(corMat, cutoff=0.5)
highly.correlated = names(cuomoData)[highCorr]
print(length(highly.correlated))
print(head(highly.correlated))
```

### Zero and near-zero variance variables
 
Identify zero and near zero variance variables.

```{r near zero variance}
nzv <- nearZeroVar(cuomoData, saveMetrics=T)
near.zero.variance = rownames(nzv[nzv$nzv==TRUE | nzv$zeroVar==TRUE,])
print(length(near.zero.variance))
print(head(near.zero.variance))
```

### Split into training and test

Remove bad variables (highly correlated and zero/near zero variance) and split into 70% training and 30% test.

```{r exclude bad features}
features.to.exclude = unique(c(highly.correlated,near.zero.variance))
print(length(features.to.exclude))
cuomoData = cuomoData[,!(colnames(cuomoData)%in%features.to.exclude)]

set.seed(42)
trainIndex <- createDataPartition(y=cuomoClass, times=1, p=0.7, list=F)
classTrain <- as.factor(cuomoClass[trainIndex])
dataTrain <- cuomoData[trainIndex,]
classTest <- as.factor(cuomoClass[-trainIndex])
dataTest <- cuomoData[-trainIndex,]
```

### Set up cross-validation

Set up 10-fold cross-validation and set seeds for all classification

```{r set seeds and train control}
set.seed(42)
seeds <- vector(mode = "list", length = 11)
for(i in 1:10) seeds[[i]] <- sample.int(1000, 75)
seeds[[11]] <- sample.int(1000,1)

train_ctrl <- trainControl(method="cv",
                           number = 10,
                           preProcOptions=list(cutoff=0.75),
                           seeds = seeds)
```

## Classifiers

### k-Nearest Neighbours

Run k-NN with k ranging from 1 to 50.

```{r kNN}
tuneParam <- data.frame(k=seq(1,50,2))
knnFit <- train(dataTrain, classTrain,
                method="knn",
                preProcess = c("center", "scale"),
                tuneGrid=tuneParam,
                trControl=train_ctrl)

knnFit
plot(knnFit)
plot(knnFit,metric="Kappa")

test_pred <- predict(knnFit, dataTest)
confusionMatrix(test_pred, classTest)
```

Identify top 10 variables in kNN.

```{r kNN features}
knnFeat = varImp(knnFit)$importance
knnFeatures = head(rownames(knnFeat[order(-rowSums(knnFeat)),]),10)
print(knnFeatures)
```

Try training kNN on top 10 features.

```{r kNN top 10 features}
dataTrain_topfeat = dataTrain[,knnFeatures]
dataTest_topfeat = dataTest[,knnFeatures]
knnFit_topfeat <- train(dataTrain_topfeat, classTrain,
               method="knn",
               tuneGrid=tuneParam,
               trControl=train_ctrl)
knnFit_topfeat
plot(knnFit_topfeat)
plot(knnFit_topfeat,metric="Kappa")

test_pred <- predict(knnFit_topfeat, dataTest_topfeat)
confusionMatrix(test_pred, classTest)
```


### Decision Tree

```{r dtree}
dtFit <- train(dataTrain, classTrain,
               method="rpart",
               tuneLength=10,
               trControl=train_ctrl)
dtFit
plot(dtFit)
plot(dtFit,metric="Kappa")

test_pred <- predict(dtFit, dataTest)
confusionMatrix(test_pred, classTest)
```

```{r dtree features}
dtFeat = varImp(dtFit)$importance
dtFeat$gene = rownames(dtFeat)
dtFeatures = head(dtFeat[order(-dtFeat$Overall),]$gene,10)
print(dtFeatures)
```

Try training decision tree on top 10 features.

```{r dtree top 10 features}
dataTrain_topfeat = dataTrain[,dtFeatures]
dataTest_topfeat = dataTest[,dtFeatures]
dtFit_topfeat <- train(dataTrain_topfeat, classTrain,
               method="rpart",
               tuneLength = 10,
               trControl=train_ctrl)
dtFit_topfeat
plot(dtFit_topfeat)
plot(dtFit_topfeat,metric="Kappa")

test_pred <- predict(dtFit_topfeat, dataTest_topfeat)
confusionMatrix(test_pred, classTest)
prp(dtFit_topfeat$finalModel)
```

### Random Forest

```{r rf}
rfFit <- train(dataTrain, classTrain,
               method="rf",
               preProcess = c("center", "scale"),
               tuneLength=10,
               trControl=train_ctrl)
rfFit
plot(rfFit)
plot(rfFit,metric="Kappa")

test_pred <- predict(rfFit, dataTest)
confusionMatrix(test_pred, classTest)

```

```{r rf features}
rfFeat = varImp(rfFit)$importance
rfFeat$gene = rownames(rfFeat)
rfFeatures = head(rfFeat[order(-rfFeat$Overall),]$gene,10)
print(rfFeatures)
varImpPlot(rfFit$finalModel,n.var = 10)
```

Try training random forest on top 10 features.

```{r rf top 10 features}
dataTrain_topfeat = dataTrain[,rfFeatures]
dataTest_topfeat = dataTest[,rfFeatures]
rfFit_topfeat <- train(dataTrain_topfeat, classTrain,
               method="rf",
               tuneLength=10,
               trControl=train_ctrl)
rfFit_topfeat
plot(rfFit_topfeat)
plot(rfFit_topfeat,metric="Kappa")

test_pred <- predict(rfFit_topfeat, dataTest_topfeat)
confusionMatrix(test_pred, classTest)
```

### Support Vector Machine

Train SVMs using linear, polynomial and radial kernels and compare the accuracy attained.

```{r svm}
L_models <- train(dataTrain, classTrain,
                  method="svmLinear",
                  preProcess = c("center", "scale"),
                  tuneLength=5,
                  trControl=train_ctrl)
P_models <- train(dataTrain, classTrain,
                  method="svmPoly",
                  preProcess = c("center", "scale"),
                  tuneLength=5,
                  trControl=train_ctrl)

R_models <- train(dataTrain, classTrain,
                  method="svmRadial",
                  preProcess = c("center", "scale"),
                  tuneLength=5,
                  trControl=train_ctrl)

resamps <- resamples(list(Linear = L_models, Poly = P_models, Radial = R_models))
summary(resamps)
bwplot(resamps, metric = "Accuracy")
densityplot(resamps, metric = "Accuracy",auto.key=TRUE)
```

```{r svm confusion}
test_pred <- predict(R_models, dataTest)
confusionMatrix(test_pred, classTest)
```

```{r svm features}
svmFeat = varImp(R_models)$importance
svmFeatures = head(rownames(svmFeat[order(-rowSums(svmFeat)),]),10)
print(svmFeatures)
```

Try training SVM on top 10 features.

```{r svm top 10 features}
dataTrain_topfeat = dataTrain[,svmFeatures]
dataTest_topfeat = dataTest[,svmFeatures]
R_models_topfeat <- train(dataTrain_topfeat, classTrain,
               method="svmRadial",
               tuneLength=10,
               trControl=train_ctrl)
R_models_topfeat
plot(R_models_topfeat)
plot(R_models_topfeat,metric="Kappa")

test_pred <- predict(R_models_topfeat, dataTest_topfeat)
confusionMatrix(test_pred, classTest)
```

```{r}
topfeat = svmFeatures[1]
secondfeat = svmFeatures[2]
plot.df = data.frame(topfeature=cuomoData[,topfeat],secondfeature=cuomoData[,secondfeat],class=as.factor(cuomoClass))
ggplot(plot.df,aes(x=topfeature,y=secondfeature,color=class))+geom_point()+xlab(topfeat)+ylab(secondfeat)+theme_bw()
```

## Conclusions

We can compare the accuracy of all the models as well as the features chosen.

```{r compare models}
resamps <- resamples(list(kNN = knnFit, Dtree = dtFit, RF = rfFit, SVM_poly = P_models, SVM_radial = R_models))
summary(resamps)
bwplot(resamps, metric = "Accuracy")
densityplot(resamps, metric = "Accuracy",auto.key=TRUE)
```

```{r compare features}
feature.list = list('kNN'=knnFeatures,'dtree'=dtFeatures,'rf'=rfFeatures,'svm'=svmFeatures)
print(upset(fromList(feature.list)))
print(intersect(knnFeatures,intersect(dtFeatures,intersect(rfFeatures,svmFeatures))))
```

```{r tsne top features}
cuomoData_topfeatures = cuomoData[,intersect(knnFeatures,intersect(dtFeatures,intersect(rfFeatures,svmFeatures)))]
cuomoTsne_topfeatures <- Rtsne(cuomoData_topfeatures, check_duplicates=FALSE, pca=TRUE, perplexity=50, theta=0.5, dims=2)
print(cuomoTsne_topfeatures$Y %>% 
  as.data.frame() %>% 
  rename(tSNE1=V1,  tSNE2=V2) %>% 
  mutate( samples=as.character(cuomoClass) )%>% 
  ggplot() +
  geom_point( mapping = aes(x=tSNE1, y=tSNE2, color=samples), alpha=0.5) +
  theme_bw())
```